{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "pd.set_option('display.max_rows',90)\n",
    "pd.set_option('display.max_columns',90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostra_hora():\n",
    "    agora = datetime.now()\n",
    "    print(agora.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 09:13:16\n",
      "2020-08-13 09:13:38\n"
     ]
    }
   ],
   "source": [
    "mostra_hora()\n",
    "from os.path import expanduser, join, abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "config = SparkConf().setAll([('spark.executor.memory', '8g'), \n",
    "                             ('spark.executor.cores', '6'), \n",
    "                             ('spark.cores.max', '6'), \n",
    "                             ('spark.driver.memory','8g'),\n",
    "                             ('spark.driver.allowMultipleContexts','true')])\n",
    "\n",
    "spark = SparkSession \\\n",
    "   .builder \\\n",
    "   .appName(\"fact_client\") \\\n",
    "   .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "   .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "   .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "   .master(\"yarn\") \\\n",
    "   .config(conf=config)  \\\n",
    "   .enableHiveSupport() \\\n",
    "   .getOrCreate()\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 09:13:39\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''set hive.exec.dynamic.partition.mode=nonstrict''')\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 09:13:39\n",
      "# safra_M01 => 202007\n",
      "# safra_M03 => 202005\n",
      "# safra_M06 => 202002\n",
      "# safra_M12 => 201908\n",
      "2020-08-13 09:13:46\n"
     ]
    }
   ],
   "source": [
    "# Definicao de variaveis\n",
    "mostra_hora()\n",
    "var_safra=202008\n",
    "\n",
    "var_safraM01 = spark.sql('''select date_format(add_months(to_date(from_unixtime(unix_timestamp(concat({var_safra},01),\"yyyyMMdd\"))),-1 ),\"yyyyMM\") as safra_m01'''.format(var_safra=var_safra)).first()[0]\n",
    "print('# safra_M01 => ' + str(var_safraM01))\n",
    "\n",
    "var_safraM03 = spark.sql('''select date_format(add_months(to_date(from_unixtime(unix_timestamp(concat({var_safra},01),\"yyyyMMdd\"))),-3 ),\"yyyyMM\") as safra_m03'''.format(var_safra=var_safra)).first()[0]\n",
    "print('# safra_M03 => ' + str(var_safraM03))\n",
    "\n",
    "var_safraM06 = spark.sql('''select date_format(add_months(to_date(from_unixtime(unix_timestamp(concat({var_safra},01),\"yyyyMMdd\"))),-6 ),\"yyyyMM\") as safra_m06'''.format(var_safra=var_safra)).first()[0]\n",
    "print('# safra_M06 => ' + str(var_safraM06))\n",
    "\n",
    "var_safraM12 = spark.sql('''select date_format(add_months(to_date(from_unixtime(unix_timestamp(concat({var_safra},01),\"yyyyMMdd\"))),-12 ),\"yyyyMM\") as safra_m12'''.format(var_safra=var_safra)).first()[0]\n",
    "print('# safra_M12 => ' + str(var_safraM12))\n",
    "\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 09:13:50\n"
     ]
    }
   ],
   "source": [
    "# Trata TBL_FACT_CLIENT\n",
    "df_fact_client=spark.sql('''\n",
    "select num_document\n",
    "      ,{var_safra} as safra\n",
    "      ,max(vl_ticket_acum_m0)  as vl_ticket_acum_m0\n",
    "      ,max(qty_ticket_acum_m0) as qty_ticket_acum_m0\n",
    "      ,max(vl_ticket_acum_m1)  as vl_ticket_acum_m1\n",
    "      ,max(qty_ticket_acum_m1) as qty_ticket_acum_m1\n",
    "      ,max(vl_ticket_acum_m3)  as vl_ticket_acum_m3\n",
    "      ,max(qty_ticket_acum_m3) as qty_ticket_acum_m3\n",
    "      ,max(vl_ticket_acum_m6)  as vl_ticket_acum_m6\n",
    "      ,max(qty_ticket_acum_m6) as qty_ticket_acum_m6\n",
    "      ,max(vl_ticket_acum_m12)  as vl_ticket_acum_m12\n",
    "      ,max(qty_ticket_acum_m12) as qty_ticket_acum_m12\n",
    "      from (\n",
    "select\n",
    "   num_document\n",
    "  ,sum(vl_ticket_m0) as vl_ticket_acum_m0\n",
    "  ,sum(qty_ticket_m0) as qty_ticket_acum_m0\n",
    "  ,NULL  as vl_ticket_acum_m1\n",
    "  ,NULL  as qty_ticket_acum_m1\n",
    "  ,NULL  as vl_ticket_acum_m3\n",
    "  ,NULL  as qty_ticket_acum_m3\n",
    "  ,NULL  as vl_ticket_acum_m6\n",
    "  ,NULL  as qty_ticket_acum_m6\n",
    "  ,NULL  as vl_ticket_acum_m12\n",
    "  ,NULL  as qty_ticket_acum_m12\n",
    "FROM db_analytics_crm.tbl_fact_client\n",
    "where safra = {var_safra}\n",
    "group by num_document\n",
    "union all\n",
    "select\n",
    "   num_document\n",
    "  ,NULL  as vl_ticket_acum_m0\n",
    "  ,NULL  as qty_ticket_acum_m0\n",
    "  ,sum(vl_ticket_m0) as vl_ticket_acum_m1\n",
    "  ,sum(qty_ticket_m0) as qty_ticket_acum_m1\n",
    "  ,NULL  as vl_ticket_acum_m3\n",
    "  ,NULL  as qty_ticket_acum_m3\n",
    "  ,NULL  as vl_ticket_acum_m6\n",
    "  ,NULL  as qty_ticket_acum_m6\n",
    "  ,NULL  as vl_ticket_acum_m12\n",
    "  ,NULL  as qty_ticket_acum_m12\n",
    "FROM db_analytics_crm.tbl_fact_client\n",
    "where safra = {var_safraM01}\n",
    "group by num_document\n",
    "union all\n",
    "select\n",
    "   num_document\n",
    "  ,NULL as vl_ticket_acum_m0\n",
    "  ,NULL as qty_ticket_acum_m0\n",
    "  ,NULL as vl_ticket_acum_m1\n",
    "  ,NULL as qty_ticket_acum_m1\n",
    "  ,sum(vl_ticket_m0)  as vl_ticket_acum_m3\n",
    "  ,sum(qty_ticket_m0)  as qty_ticket_acum_m3\n",
    "  ,NULL  as vl_ticket_acum_m6\n",
    "  ,NULL  as qty_ticket_acum_m6\n",
    "  ,NULL  as vl_ticket_acum_m12\n",
    "  ,NULL  as qty_ticket_acum_m12\n",
    "FROM db_analytics_crm.tbl_fact_client\n",
    "where safra between {var_safraM03} and {var_safraM01}\n",
    "group by num_document\n",
    "union all\n",
    "select\n",
    "   num_document\n",
    "  ,NULL as vl_ticket_acum_m0\n",
    "  ,NULL as qty_ticket_acum_m0\n",
    "  ,NULL as vl_ticket_acum_m1\n",
    "  ,NULL as qty_ticket_acum_m1\n",
    "  ,NULL  as vl_ticket_acum_m3\n",
    "  ,NULL  as qty_ticket_acum_m3\n",
    "  ,sum(vl_ticket_m0)  as vl_ticket_acum_m6\n",
    "  ,sum(qty_ticket_m0)  as qty_ticket_acum_m6\n",
    "  ,NULL  as vl_ticket_acum_m12\n",
    "  ,NULL  as qty_ticket_acum_m12\n",
    "FROM db_analytics_crm.tbl_fact_client\n",
    "where safra between {var_safraM06} and {var_safraM01}\n",
    "group by num_document\n",
    "union all\n",
    "select\n",
    "   num_document\n",
    "  ,NULL as vl_ticket_acum_m0\n",
    "  ,NULL as qty_ticket_acum_m0\n",
    "  ,NULL as vl_ticket_acum_m1\n",
    "  ,NULL as qty_ticket_acum_m1\n",
    "  ,NULL  as vl_ticket_acum_m3\n",
    "  ,NULL  as qty_ticket_acum_m3\n",
    "  ,NULL  as vl_ticket_acum_m6\n",
    "  ,NULL  as qty_ticket_acum_m6\n",
    "  ,sum(vl_ticket_m0)  as vl_ticket_acum_m12\n",
    "  ,sum(qty_ticket_m0)  as qty_ticket_acum_m12\n",
    "FROM db_analytics_crm.tbl_fact_client\n",
    "where safra between {var_safraM12} and {var_safraM01}\n",
    "group by num_document\n",
    ") \n",
    "group by num_document\n",
    "'''.format(var_safra=var_safra\n",
    "          ,var_safraM01=var_safraM01\n",
    "          ,var_safraM03=var_safraM03\n",
    "          ,var_safraM06=var_safraM06\n",
    "          ,var_safraM12=var_safraM12))\n",
    "df_fact_client.createOrReplaceTempView('tb_df_fact_client_acum')\n",
    "print('#')\n",
    "print('# Dados da FACT_CLIENT processada')\n",
    "print('#')\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "spark.sql('''\n",
    "desc tb_fact_client_acum_temp\n",
    "''').show()\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mostra_hora()\n",
    "spark.sql('''\n",
    "drop table if exists db_analytics_crm.tbl_fact_client_acum_temp4\n",
    "''')\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "mostra_hora()\n",
    "spark.sql('''\n",
    "select safra, count(*)\n",
    "from tb_fact_client_acum_temp\n",
    "group by safra\n",
    "order by safra desc\n",
    "''').show(100,truncate=False)\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Tabela dim_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 09:13:50\n",
      "# Processando safra 202008\n",
      "2020-08-13 09:13:54\n"
     ]
    }
   ],
   "source": [
    "mostra_hora()\n",
    "print('#')\n",
    "print('# Processando safra ' + str(var_safra))\n",
    "\n",
    "df_join_product=spark.sql('''select\n",
    "   num_document\n",
    "  ,safra as safra\n",
    "  ,nvl(max(tot_categ_food),0)     as tot_categ_food\n",
    "  ,nvl(max(tot_categ_nfood),0)    as tot_categ_nfood\n",
    "  ,nvl(max(vl_tot_categ_food),0)  as vl_tot_categ_food\n",
    "  ,nvl(max(vl_tot_categ_nfood),0) as vl_tot_categ_nfood\n",
    "from(\n",
    "select\n",
    "          it.num_document, it.safra\n",
    "         ,COUNT(DISTINCT CASE WHEN UPPER(NVL(pd.cod_bus,\"FOOD\")) <> \"NFOOD\" THEN\n",
    "                                 NVL(pd.ds_category,1)\n",
    "         END)                                    as tot_categ_food\n",
    "         ,COUNT(DISTINCT CASE WHEN UPPER(NVL(pd.cod_bus,\"FOOD\")) = \"NFOOD\" THEN\n",
    "                                   NVL(pd.ds_category,1)\n",
    "         END)                                    as tot_categ_nfood\n",
    "         ,SUM(CASE WHEN UPPER(NVL(pd.cod_bus,\"FOOD\")) <> \"NFOOD\" THEN \n",
    "                        it.vl_item\n",
    "                   END)                          AS vl_tot_categ_food\n",
    "         ,SUM(CASE WHEN UPPER(NVL(pd.cod_bus,\"FOOD\")) = \"NFOOD\" THEN \n",
    "                        it.vl_item\n",
    "                   END)                          AS vl_tot_categ_nfood\n",
    "      from db_analytics_crm.tbl_fact_sale_item it\n",
    "      left join\n",
    "      db_analytics_crm.tbl_dim_product pd\n",
    "        on it.safra = pd.safra\n",
    "       and it.cod_product_rms = pd.cod_product_rms\n",
    "      where it.safra = {var_safra}\n",
    "      group by IT.num_document, it.safra) a\n",
    "group by num_document, safra'''.format(var_safra=var_safra))\n",
    "\n",
    "df_join_product.createOrReplaceTempView('tb_df_join_product')\n",
    "\n",
    "print('#')\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.sql('''select * from tb_df_join_product limit 20''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tabela_dim_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.sql('''\n",
    "create table if not exists db_analytics_crm.tb_ind_item_site_temp5\n",
    "(num_document         decimal(38,18)\n",
    ",tot_gasto_hiper_m0   decimal(38,18)\n",
    ",tot_gasto_market_m0  decimal(38,18)\n",
    ",tot_gasto_bairro_m0  decimal(38,18)\n",
    ",tot_gasto_express_m0 decimal(38,18)\n",
    ",tot_gasto_ecom_m0    decimal(38,18)\n",
    ",tot_gasto_posto_m0   decimal(38,18)\n",
    ",tot_gasto_drogaria_m0 decimal(38,18)\n",
    ") stored as orc\n",
    "partitioned by (safra int)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 09:13:54\n",
      "# Processando safra 202008\n",
      "# Fim\n",
      "2020-08-13 09:13:56\n"
     ]
    }
   ],
   "source": [
    "# processa safra específica\n",
    "mostra_hora()\n",
    "print('#')\n",
    "print('# Processando safra ' + str(var_safra))\n",
    "\n",
    "df_join_sites=spark.sql('''\n",
    "select\n",
    "   item.num_document\n",
    "  ,{var_safra} as safra\n",
    "  ,sum(CASE WHEN UPPER(loja.ds_site_format) = \"LOJA HIPERMERCADO\" OR \n",
    "                 UPPER(loja.ds_site_format) = \"HIPERMERCADOS\" THEN\n",
    "                 item.vl_item\n",
    "        END)                                             AS tot_gasto_hiper_m0\n",
    "  ,sum(CASE WHEN UPPER(loja.ds_site_format) = 'SUPERMERCADOS' AND \n",
    "                 UPPER(loja.ds_site) LIKE '%MARKET%' AND \n",
    "                 loja.ini_site <> 'MVA' THEN \n",
    "                 item.vl_item\n",
    "        END)                                             AS tot_gasto_market_m0\n",
    "  ,sum(CASE WHEN (UPPER(loja.ds_site_format) = 'SUPERMERCADOS' AND \n",
    "                  UPPER(loja.ds_site) NOT LIKE '%MARKET%') \n",
    "                  OR\n",
    "                  UPPER(loja.ini_site) = 'MVA' THEN\n",
    "                  item.vl_item\n",
    "                  END)                                        as tot_gasto_bairro_m0\n",
    "  ,sum(CASE WHEN UPPER(loja.ds_site_format) = \"EXPRESS\" THEN\n",
    "                 item.vl_item  \n",
    "        END)                                             AS tot_gasto_express_m0\n",
    "  ,sum(CASE WHEN UPPER(loja.ds_site_format) = \"E-COMMERCE\" THEN\n",
    "                 item.vl_item\n",
    "        END)                                             AS tot_gasto_ecom_m0\n",
    "  ,sum(CASE WHEN UPPER(loja.ds_site_format) = \"POSTOS\" THEN\n",
    "                 item.vl_item\n",
    "        END)                                             AS tot_gasto_posto_m0\n",
    "  ,sum(CASE WHEN UPPER(loja.ds_site_format) = \"DROGARIA\" THEN\n",
    "                 item.vl_item\n",
    "        END)                                             AS tot_gasto_drogaria_m0      \n",
    "from  db_analytics_crm.tbl_fact_sale_item item\n",
    "left join\n",
    "db_analytics_crm.tbl_dim_site LOJA\n",
    " on loja.safra = item.safra\n",
    "and item.cod_site = loja.cod_site\n",
    "where item.safra = {var_safra}\n",
    "group by item.num_document, item.safra\n",
    "'''.format(var_safra=var_safra))\n",
    "\n",
    "df_join_sites.createOrReplaceTempView('tb_itens_site_temp')\n",
    "\n",
    "print('# Fim')\n",
    "print('#')\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.sql('''select * from tb_itens_site_temp limit 20''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# tabela_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mostra_hora()\n",
    "spark.sql('''\n",
    "create TABLE if not exists db_analytics_crm.tbl_indicadores_fact_client\n",
    "(num_document               bigint\n",
    ",gasto_medio_m0             decimal(38,18)\n",
    ",ticket_medio_m0            decimal(38,18)\n",
    ",tot_gasto_alimentar        decimal(38,18)\n",
    ",tot_gasto_n_alimentar      decimal(38,18)\n",
    ",qtd_categorias_compradas   bigint \n",
    ",qtd_categorias_alimentar   bigint \n",
    ",qtd_catgorias_n_alimentar  bigint\n",
    ",tot_gasto_hiper            decimal(38,18)\n",
    ",tot_gasto_market           decimal(38,18)\n",
    ",tot_gasto_bairro           decimal(38,18)\n",
    ",tot_gasto_express          decimal(38,18)\n",
    ",tot_gasto_ecom             decimal(38,18)\n",
    ",tot_gasto_posto            decimal(38,18)\n",
    ",tot_gasto_drogaria         decimal(38,18) \n",
    ",tot_compras_m01            decimal(38,18)\n",
    ",qtd_compras_m01            bigint\n",
    ",tot_compras_m03            decimal(38,18)\n",
    ",qtd_compras_m03            bigint\n",
    ",tot_compras_m06            decimal(38,18)\n",
    ",qtd_compras_m06            bigint\n",
    ",tot_compras_m12            decimal(38,18)\n",
    ",qtd_compras_m12            bigint\n",
    ",dt_geracao_dado            date\n",
    ") PARTITIONED BY ( safra int)\n",
    "STORED AS ORC\n",
    "TBLPROPERTIES (\n",
    "\"orc.compress\"=\"SNAPPY\",\n",
    "\"orc.compress.size\"=\"262144\",\n",
    "\"orc.create.index\"=\"true\",\n",
    "\"orc.row.index.stride\"=\"10000\",\n",
    "\"orc.stripe.size\"=\"671088640\",\n",
    "\"orc.bloom.filter.fpp\"=\"0.01\",\n",
    "\"orc.bloom.filter.columns\"=\"num_document\"\n",
    ")\n",
    "''')\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 09:13:56\n",
      "# Gravando safra 202008\n",
      "# Fim\n",
      "2020-08-13 09:23:25\n"
     ]
    }
   ],
   "source": [
    "mostra_hora()\n",
    "\n",
    "print('# Gravando safra ' + str(var_safra))\n",
    "\n",
    "spark.sql('''\n",
    "insert overwrite table db_analytics_crm.tbl_indicadores_fact_client partition (safra)\n",
    "select \n",
    "   cli.num_document\n",
    "  ,cli.vl_ticket_acum_m0                          as gasto_medio_m0\n",
    "  ,cli.vl_ticket_acum_m0 / cli.qty_ticket_acum_m0 as ticket_medio_m0\n",
    "  ,prd.vl_tot_categ_food                          as tot_gasto_alimentar \n",
    "  ,prd.vl_tot_categ_nfood                         as tot_gasto_n_alimentar\n",
    "  ,(prd.tot_categ_food + prd.tot_categ_nfood)     as qtd_categorias_compradas\n",
    "  ,prd.tot_categ_food                             as qtd_categorias_alimentar\n",
    "  ,prd.tot_categ_nfood                            as qtd_catgorias_n_alimentar \n",
    "  ,ste.tot_gasto_hiper_m0                         as tot_gasto_hiper  \n",
    "  ,ste.tot_gasto_market_m0                        as tot_gasto_market\n",
    "  ,ste.tot_gasto_bairro_m0                        as tot_gasto_bairro\n",
    "  ,ste.tot_gasto_express_m0                       as tot_gasto_express\n",
    "  ,ste.tot_gasto_ecom_m0                          as tot_gasto_ecom\n",
    "  ,ste.tot_gasto_posto_m0                         as tot_gasto_posto\n",
    "  ,ste.tot_gasto_drogaria_m0                      as tot_gasto_drogaria\n",
    "  ,NVL(cli.vl_ticket_acum_m1  ,0)                 as tot_compras_m01  \n",
    "  ,NVL(cli.qty_ticket_acum_m1 ,0)                 as qtd_compras_m01  \n",
    "  ,NVL(cli.vl_ticket_acum_m3  ,0)                 as tot_compras_m03  \n",
    "  ,NVL(cli.qty_ticket_acum_m3 ,0)                 as qtd_compras_m03 \n",
    "  ,NVL(cli.vl_ticket_acum_m6  ,0)                 as tot_compras_m06  \n",
    "  ,NVL(cli.qty_ticket_acum_m6 ,0)                 as qtd_compras_m06 \n",
    "  ,NVL(cli.vl_ticket_acum_m12 ,0)                 as tot_compras_m12 \n",
    "  ,NVL(cli.qty_ticket_acum_m12,0)                 as qtd_compras_m12\n",
    "  ,current_date                                   as dt_geracao_dado\n",
    "  ,cli.safra\n",
    "from tb_df_fact_client_acum cli\n",
    "left join\n",
    "tb_df_join_product prd\n",
    " on cli.num_document = prd.num_document\n",
    "and cli.safra = prd.safra\n",
    "left join\n",
    "tb_itens_site_temp ste\n",
    " on cli.num_document = ste.num_document\n",
    "and cli.safra = ste.safra\n",
    "where cli.safra = {var_safra}\n",
    "'''.format(var_safra=var_safra))\n",
    "print('# Fim')\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mostra_hora()\n",
    "#202001|27070646\n",
    "spark.sql('''select safra, count(*) as total from db_analytics_crm.tbl_indicadores_fact_client\n",
    "group by safra\n",
    "order by safra \n",
    "''').show(truncate=False)\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-13 09:23:25\n",
      "+------+-------------------------------------------------------+\n",
      "|safra |(sum(tot_gasto_alimentar) + sum(tot_gasto_n_alimentar))|\n",
      "+------+-------------------------------------------------------+\n",
      "|201901|1510125309.88000000000000000                           |\n",
      "|201902|1483888564.74000000000000000                           |\n",
      "|201903|1703212880.98000000000000000                           |\n",
      "|201904|1711200388.07000000000000000                           |\n",
      "|201905|1660992401.14000000000000000                           |\n",
      "|201906|1580598194.93000000000000000                           |\n",
      "|201907|1572398592.13000000000000000                           |\n",
      "|201908|1687727046.80000000000000000                           |\n",
      "|201909|1562191615.26000000000000000                           |\n",
      "|201910|1648707471.70000000000000000                           |\n",
      "|201911|1933794962.77000000000000000                           |\n",
      "|201912|2263517665.87000000000000000                           |\n",
      "|202001|1641488929.60000000000000000                           |\n",
      "|202002|1663506239.29000000000000000                           |\n",
      "|202003|1812593094.15000000000000000                           |\n",
      "|202004|1858584433.60000000000000000                           |\n",
      "|202005|2120351887.61000000000000000                           |\n",
      "|202006|1865994680.01000000000000000                           |\n",
      "|202007|1881646936.54000000000000000                           |\n",
      "|202008|580928653.85000000000000000                            |\n",
      "+------+-------------------------------------------------------+\n",
      "\n",
      "2020-08-13 09:24:47\n"
     ]
    }
   ],
   "source": [
    "# 202004|1858584433.60000000000000000                           |\n",
    "# 202008|274008581.08000000000000000 \n",
    "# 202008|357915156.66000000000000000 \n",
    "# 202008|370752372.75000000000000000 \n",
    "# 202008|580928653.85000000000000000\n",
    "mostra_hora()\n",
    "spark.sql('''\n",
    "select safra,(sum(tot_gasto_alimentar) + sum(tot_gasto_n_alimentar))\n",
    "from db_analytics_crm.tbl_indicadores_fact_client\n",
    "where safra between 201901 and 202008\n",
    "group by safra\n",
    "order by safra\n",
    "'''.format(var_safra=var_safra)).show(truncate=False)\n",
    "mostra_hora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
